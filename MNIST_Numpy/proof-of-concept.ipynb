{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "span = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for initializing network weights\n",
    "class Linear_layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # forward pass variables\n",
    "        self.W = random.rand(output_size, input_size)\n",
    "        self.B = random.rand(1)\n",
    "        self.A0 = np.zeros((input_size, 1))\n",
    "\n",
    "        # gradients\n",
    "        self.dA = np.zeros((input_size, 1))\n",
    "        self.dW = np.zeros((output_size, input_size))\n",
    "        self.dB = np.zeros(1)\n",
    "        self.dZ = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, A0):\n",
    "        # copy the input and store it for gradient computation later\n",
    "        self.A0 = A0\n",
    "\n",
    "        # forward pass\n",
    "        U = np.matmul(self.W, self.A0)\n",
    "        Z = U + self.B\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # compute gradients given the dZ\n",
    "        self.dB = np.sum(dZ) / dZ.size\n",
    "        self.dA = np.matmul(np.transpose(self.W), dZ)\n",
    "        self.dW = np.matmul(dZ, np.transpose(self.A0))\n",
    "\n",
    "        # return out dA (ideally to be passed to activation function's backward)\n",
    "        return self.dA\n",
    "\n",
    "    def update_weights(self, lr=0.001):\n",
    "        # update weights\n",
    "        self.W -= lr * self.dW\n",
    "        self.B -= lr * self.dB\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for ReLU Activation\n",
    "class ReLU_layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    def backward(self, input, dP):\n",
    "        return 1. * (input > 0) * dP\n",
    "\n",
    "# class for Sigmoid Activation\n",
    "class Softmax_layer:\n",
    "    def __init__(self):\n",
    "        self.output = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = np.exp(input - np.max(input))\n",
    "        self.output = output / np.sum(output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dL):\n",
    "        # https://e2eml.school/softmax.html\n",
    "        grad = self.output * np.identity(self.output.size) - np.matmul(self.output, np.transpose(self.output))\n",
    "        return np.matmul(grad, dL)\n",
    "\n",
    "# class for calculating loss function\n",
    "class Log_loss:\n",
    "    def __init__(self):\n",
    "        self.loss = 0\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        # labels must be -1 to 1\n",
    "        self.loss = - (np.sum(labels * np.log(input)) + np.sum((1 - labels) * np.log(1 - input)))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, input, labels):\n",
    "        term1 = - labels / input\n",
    "        term2 = - (1 - labels) / (1 - input)\n",
    "\n",
    "        return term1 + term2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the neural network\n",
    "layer1 = Linear_layer(span, 10)\n",
    "activation1 = ReLU_layer()\n",
    "layer2 = Linear_layer(10, span)\n",
    "activation2 = ReLU_layer()\n",
    "output = Softmax_layer()\n",
    "loss = Log_loss()\n",
    "\n",
    "def generate_input(span):\n",
    "    idx = random.randint(span)\n",
    "    input = np.ones((span, 1))\n",
    "    input[idx] = 0.\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop training\n",
    "for i in range(50000):\n",
    "    # generate random input\n",
    "    input = generate_input(span)\n",
    "\n",
    "    # generate label\n",
    "    label = 1. - input\n",
    "\n",
    "    # feedforward\n",
    "    inter1 = layer1.forward(input)\n",
    "    output1 = activation1.forward(inter1)\n",
    "    inter2 = layer2.forward(output1)\n",
    "    output2 = activation2.forward(inter2)\n",
    "    prediction = output.forward(output2)\n",
    "\n",
    "    # generate loss\n",
    "    prediction_loss = loss.forward(prediction, label)\n",
    "    # backward through loss\n",
    "    dL = loss.backward(prediction, label)\n",
    "\n",
    "    # backward through network\n",
    "    dP = output.backward(dL)\n",
    "    dZ2 = activation2.backward(output2, dP)\n",
    "    dA2 = layer2.backward(dZ2)\n",
    "    dZ1 = activation1.backward(output1, dA2)\n",
    "    _ = layer1.backward(dZ1)\n",
    "\n",
    "    # update weights\n",
    "    layer1.update_weights()\n",
    "    layer2.update_weights()\n",
    "\n",
    "    # print(prediction_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0 0.7917846050124938\n2 2 0.7897085308460833\n9 9 0.7469796570571245\n7 7 0.8177294931989231\n2 2 0.7897085308460833\n6 6 0.8323295161251786\n3 3 0.7639276970608413\n5 5 0.7389320030264476\n2 2 0.7897085308460833\n5 5 0.7389320030264476\n9 9 0.7469796570571245\n3 3 0.7639276970608413\n7 7 0.8177294931989231\n3 3 0.7639276970608413\n1 1 0.7527311936831417\n"
     ]
    }
   ],
   "source": [
    "# loop inference\n",
    "for i in range(15):\n",
    "    # generate random input\n",
    "    input = generate_input(span)\n",
    "\n",
    "    # feedforward\n",
    "    inter1 = layer1.forward(input)\n",
    "    output1 = activation1.forward(inter1)\n",
    "    inter2 = layer2.forward(output1)\n",
    "    output2 = activation2.forward(inter2)\n",
    "    prediction = output.forward(output2)\n",
    "\n",
    "    print(np.argmin(input), np.argmax(prediction), np.max(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}